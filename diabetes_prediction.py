# -*- coding: utf-8 -*-
"""Diabetes Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1urv68WUkPVpnLdLJGCwTcM1UL4HhO8St
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler

import plotly.graph_objects as go
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LinearRegression
from lazypredict.Supervised import LazyClassifier

# Load the dataset
data = pd.read_csv(r"C:\Users\Medo\Desktop\DS\diabetes_prediction\diabetes.csv", na_values=['?', 'UNDEFIND'])

data.head().T.style.set_properties(**{'background-color': 'grey',
                                      'color': 'white',
                                      'border-color': 'white'})

"""**Data Dictionary**

---


      Columns         	Description


---


    Pregnancies: 	  To express the Number of pregnancies
    Glucose: 	      To express the Glucose level in blood
    BloodPressure: 	To express the Blood pressure measurement
    SkinThickness: 	To express the thickness of the skin
    Insulin:       	To express the Insulin level in blood
    BMI: 	          To express the Body mass index
    DiabetesPedigreeFunction: 	To express the Diabetes percentage
    Age: 	          To express the age
    Outcome:       	To express the final result 1 is Yes and 0 is No

# *** Preprocessing  Section***:

Check null value counts
"""

data.isnull().sum()

"""

Check duplicated data
"""

data.duplicated()

# Check if Glucose contains a zero value
data.Glucose.unique()

"""The glucose value cannot be zero, so replacing the zero values with the mean of the glucose column

"""

data.Glucose = data.Glucose.replace(0, int(data.Glucose.mean()))

# Check if BloodPressure contains a zero value
data.BloodPressure.unique()

"""
Filling the zero values with the median( Blood pressure column)



"""

data.BloodPressure.replace(0, data.BloodPressure.median(), inplace=True)

# Check if SkinThickness contains a zero value

data.SkinThickness.unique()

"""Filling the zero values with the median( SkinThickness column)


"""

data.SkinThickness.replace(0, data.SkinThickness.median(), inplace=True)

# Check if Insulin contains a zero value

data.Insulin.unique()

"""Filling the zero values with the median( Insulin column)


"""

data.Insulin.replace(0, data.Insulin.median(), inplace=True)

# Check if BMI contains a zero value

data.BMI.unique()

"""#Filling the zero values with the median( BMI column)


"""

data.BMI.replace(0, data.BMI.mean(), inplace=True)

# Check if DiabetesPedigreeFunction contains a zero value
data.DiabetesPedigreeFunction.unique()

# Check if Age contains a zero value

data.Age.unique()

"""#Check if data contains a zero value 

"""

checkNullData = data.drop("Outcome", axis=1)
print(checkNullData.isnull().sum())
checkNullData.isnull().head()

"""#Prediction using Logistic Regression"""

scaler = StandardScaler()

# this code will print the names of all data columns
data.columns

"""# Split the data into a training set and a test set"""

X = data.drop(columns=["Outcome", "Insulin", "BMI", "DiabetesPedigreeFunction"])
X = scaler.fit_transform(X)

y = data["Outcome"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.12)

lm = LogisticRegression()
lm.fit(X_train, y_train)

y_predictions = lm.predict(X_test)

X_train, X_test, y_train, y_test = train_test_split(data.drop('Outcome', axis=1),
                                                    data['Outcome'], test_size=0.2,
                                                    random_state=42)

clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)
models, predictions = clf.fit(X_train, X_test, y_train, y_test)

models.sort_values('Accuracy', ascending=False)

# Calculate the mean accuracy of the models
mean_accuracy = models['Accuracy'].mean()

# Calculate the median accuracy of the models
median_accuracy = models['Accuracy'].median()

# Print the mean and median accuracy
print("Mean accuracy:", mean_accuracy)
print("Median accuracy:", median_accuracy)

"""#Check the Performance of the model """

precision = metrics.precision_score(y_test, y_predictions)
accuracy = metrics.accuracy_score(y_test, y_predictions)

recall = metrics.recall_score(y_test, y_predictions)
confusion_matrix(y_test, y_predictions)
f1score = metrics.f1_score(y_test, y_predictions)

print("Accuracy:", accuracy)
print("recall:", recall)
print("f1score:", f1score)
print("precision:", precision)

# Separate the features and the target variable
X = data.drop("Outcome", axis=1)
y = data["Outcome"]

# Split the data into a training set and a test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train the model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
'''
x_train, x_test, y_train, y_test = train_test_split(data, y, test_size=0.10, random_state=2)
scaler = MinMaxScaler()
scaler.fit(x_train)

x_train = scaler.transform(x_train)
x_train
x_test = scaler.transform(x_test)
x_train = pd.DataFrame(x_train, columns=data.columns)
x_test = pd.DataFrame(x_test, columns=data.columns)

x_train

reg = LinearRegression()
reg.fit(x_train, y_train)

from sklearn import metrics

print("mean_absolute_error", metrics.mean_absolute_error(y_test, y_predict))
print("mean_squared_error", metrics.mean_squared_error(y_test, y_predict))
print("root_mean_squared_error", np.sqrt(metrics.mean_squared_error(y_test, y_predict)))

from sklearn.tree import DecisionTreeRegressor

dtree = DecisionTreeRegressor(max_depth=12, random_state=3)
dtree.fit(x_train, y_train)
y_predict_tree = dtree.predict(x_test)
res = pd.DataFrame({'Actual': y_test, 'Predicted': y_predict_tree})
res

# Plot the actual and predicted values
plt.scatter(y_test, y_predict_tree)

# Add a line of best fit
plt.plot(y_test, y_predict_tree, color='red')

# Show the plot
plt.show()

colors = ['gold', 'mediumturquoise']
labels = ['Actual', 'Predicted']
values = y_test.value_counts() / y_predict_tree.shape[0]

# Use `hole` to create a donut-like pie chart
fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])
fig.update_traces(hoverinfo='label+percent', textinfo='percent', textfont_size=20,
                  marker=dict(colors=colors, line=dict(color='#000000', width=2)))
fig.update_layout(
    title_text="Outcome")
fig.show()

plt.scatter(y_test, y_predict_tree)
plt.show()
'''''